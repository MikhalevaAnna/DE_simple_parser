# Разработка простого парсера

## Задание:
Если парсить нужно что-то маленькое и простое, то в большинстве случаев с задачей прекрасно справится дуэт из `Requests` и `BeautifulSoup`. </br>
1. Выберите любой из предложенных ниже сайтов для парсинга (выбрала `books.toscrape.com`).
2. Напишите кастомную конфигурацию, в которую будут ходить настройки заголовка, задержи между запросами, количество повторных попыток при статус-коде != 200.
### Требования к выполнению задания:
- Парсер собирает минимум три поля информации;
- Парсер обрабатывает ошибки соедниения;
- Парсер работает с пагинацией: обходит все страницы каталога
- Парсер сохраняет все данные в csv, с помощью `Pandas/PySpark` проводится агрегация, проверка на целостность данных
- Фильтрованные данные загружаются в БД: `Postgres/Хранилище Selectel`
### Требования к коду:
- Используйте функции для модульности и читаемые названия переменных
- Комментируйте сложные участки кода
- Проверьте Ваш код на соответствие `PEP8` (можно через `codestyle`)</br>

Задание будет считаться выполненным, если соблюдены все требования к коду, а также выполнено от 3 до 5 заданий, количество баллов за задание выдается соответственно.</br>

## Реализация:
Для парсинга выбран сайт `books.toscrape.com`. </br>

1. Парсер собирает информацию по следующим полям:
   - `title` - название книги,
   - `price` - стоимость,
   - `rating` - рейтинг,
   - `stock` - количество книг в наличии,
   - `in_stock` - в наличии/отсутствует,
   - `url` - ссылка на карточку книги. </br> 

3. Парсер обрабатывает ошибки соединения:
- В функции `make_request`: реализована обработка ошибок с повторными попытками (`max_retries`)
- В функции `parse_book_card`: обработка ошибок парсинга отдельных карточек
- В функции `scrape_page`: проверка ответа от сервера

3. Парсер работает с пагинацией, обходит все страницы каталога:
- В функции scrape_all_pages: реализован обход всех страниц через поиск ссылки "next"
- Логика: парсит текущую страницу → находит ссылку на следующую → продолжает

4. Парсер сохраняет все данные в CSV, проводится агрегация и проверка целостности:
- Сохранение в CSV: save_data_locally()
- Агрегация: analyze_and_save_stats() - статистика по ценам, рейтингам, наличие
- Проверка целостности: проверка пропущенных значений, дубликатов, выбросов

5. Фильтрованные данные загружаются в БД/хранилище:
- Загрузка в S3 Selectel реализована через boto3
- Функции: upload_df_to_s3(), upload_file_to_s3()
- Структура папок: raw_data/, cleaned_data/, statistics/, filtered_data/

✅ Требования к коду:
1. Используйте функции для модульности и читаемые названия переменных:
- Модульная структура: функции сгруппированы по назначению (S3, парсинг, анализ)
- Читаемые названия: scrape_all_pages, parse_book_card, analyze_and_save_stats

2. Комментируйте сложные участки кода:
- Документационные строки: у каждой функции есть описание
- Комментарии: в сложных местах (обработка относительных ссылок, работа с S3)

3. Проверьте Ваш код на соответствие PEP8:
- Соблюдение отступов: 4 пробела
- Длина строк: в основном соблюдается 79 символов
- Именование: snake_case для функций и переменных
- Импорты: сгруппированы и упорядочены

4. Осуществляется расширенный анализ данных:
- Считается статистика по рейтингам и ценам
- Обнаруживаются выбросы
- Проверяется качество данных
- Удаляются полные дубли
