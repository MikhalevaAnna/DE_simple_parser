"""
–ü–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–∞ books.toscrape.com —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ S3 Selectel –±–∞–∫–µ—Ç.
"""
import concurrent.futures
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re
from urllib.parse import urljoin
from typing import List, Dict, Optional, Tuple, Any
import os
from dotenv import load_dotenv

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
from src.constants import (CUSTOM_CONFIG, LOG_ENCODING,
                           RATING_MAP, DEFAULT_RATING,
                           LOCAL_CSV_DEFAULT, DEFAULT_ENCODING,
                           MAX_WORKERS_VALUE, BASE_URL,
                           LOCAL_CSV_ALL_BOOKS, CSV_ALL_BOOKS,
                           LOGS_DIR, FILES_DIR, BASE_DIR)
from src.s3_interaction import initialize_s3, save_data_to_s3
from src.data_analyze import analyze_and_save_stats, filter_and_save_books
from src.logger import setup_logging

load_dotenv()
logger = setup_logging()

# ========== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ==========


def get_scraper_config() -> Dict[str, Any]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∑–∞–ø—Ä–æ—Å–æ–≤.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø–∞—Ä—Å–µ—Ä–∞
    """
    return CUSTOM_CONFIG


# ========== –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –ü–ê–†–°–ï–†–ê ==========

def make_request(url: str,
                 config: Dict[str, Any],
                 session: requests.Session
                 ) -> Optional[requests.Response]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP-–∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏.

    Args:
        url: URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ –∏–∑ get_scraper_config().
        session: –°–µ—Å—Å–∏—è requests –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π.

    Returns:
        Response –æ–±—ä–µ–∫—Ç –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ,
        None –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫.
    """

    for attempt in range(config['max_retries'] + 1):
        try:
            logger.debug(f"–ó–∞–ø—Ä–æ—Å –∫ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
            response = session.get(url, timeout=config['timeout'])

            if response.encoding is None or response.encoding == 'ISO-8859-1':
                response.encoding = LOG_ENCODING

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–¥
            if response.status_code == 200:
                return response
            elif 500 <= response.status_code < 600:
                logger.warning(f"–°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {response.status_code} –¥–ª—è {url}")
            else:
                logger.warning(f"–ö–ª–∏–µ–Ω—Ç—Å–∫–∞—è –æ—à–∏–±–∫–∞ {response.status_code} –¥–ª—è {url}")
                return None  # –ù–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º –¥–ª—è –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö –æ—à–∏–±–æ–∫

        except requests.exceptions.ConnectionError as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è {url}: {e}")

            # –î–µ—Ç–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫—É —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            if "Name or service not known" in str(e):
                logger.error(f"DNS –æ—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑—Ä–µ—à–∏—Ç—å –∏–º—è —Ö–æ—Å—Ç–∞ –¥–ª—è {url}")
            elif "timed out" in str(e).lower():
                logger.error(f"–¢–∞–π–º–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è {url}")
            elif "refused" in str(e).lower():
                logger.error(f"–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–æ –¥–ª—è {url} (–ø–æ—Ä—Ç –∑–∞–∫—Ä—ã—Ç)")

        except requests.exceptions.Timeout as e:
            logger.error(f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {url}: {e}")

        except requests.exceptions.TooManyRedirects as e:
            logger.error(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –¥–ª—è {url}: {e}")
            return None  # –ù–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º

        except requests.exceptions.HTTPError as e:
            logger.error(f"HTTP –æ—à–∏–±–∫–∞ –¥–ª—è {url}: {e}")

        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {url}: {e}")

        # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
        if attempt < config['max_retries']:
            delay = config['min_delay'] * (2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            delay = min(delay, config['max_delay'] * 3)
            logger.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {delay:.2f} —Å–µ–∫—É–Ω–¥...")
            time.sleep(delay)

    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ {url} –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫")
    return None


def parse_book_card(card: BeautifulSoup, base_url: str
                    ) -> Optional[Dict[str, Any]]:
    """
    –ü–∞—Ä—Å–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ –∏–∑ HTML-–∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∫–∞—Ç–∞–ª–æ–≥–∞.

    Args:
        card: BeautifulSoup –æ–±—ä–µ–∫—Ç –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–Ω–∏–≥–∏.
        base_url: –ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–Ω–∏–≥–µ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ:
        {
            'title': str - –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏,
            'price': float - —Ü–µ–Ω–∞ –≤ —Ñ—É–Ω—Ç–∞—Ö,
            'rating': int - —Ä–µ–π—Ç–∏–Ω–≥ –æ—Ç 0 –¥–æ 5,
            'stock': int - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –Ω–∞–ª–∏—á–∏–∏,
            'in_stock': bool - –Ω–∞–ª–∏—á–∏–µ –≤ –º–∞–≥–∞–∑–∏–Ω–µ,
            'url': str - URL –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        }
    """
    try:
        book_info = {}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
        title_elem = card.find('h3').find('a')
        book_info['title'] = title_elem.get('title',
                                            title_elem.text.strip())

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É
        price_elem = card.find('p', class_='price_color')
        price_text = price_elem.text.strip() if price_elem else ''
        price_match = re.search(r'[\d.]+', price_text)
        book_info['price'] = (float(price_match.group())
                              if price_match
                              else 0.0)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥
        rating_elem = card.find('p', class_='star-rating')
        rating_classes = (rating_elem.get('class', [])
                          if rating_elem
                          else [])

        for cls in rating_classes:
            if cls in RATING_MAP:
                book_info['rating'] = RATING_MAP[cls]
                break
        else:
            book_info['rating'] = DEFAULT_RATING

        # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å –Ω–∞–ª–∏—á–∏–µ–º –∫–Ω–∏–≥
        stock_elem = card.find('p', class_='instock availability')

        if not stock_elem:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–∞
            stock_elem = card.find('p', class_='availability')

        stock_text = stock_elem.text.strip() if stock_elem else ''

        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —á–∏—Å–ª–æ –≤ —Ç–µ–∫—Å—Ç–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä In stock (19 available))
        stock_match = re.search(r'\((\d+)\s+available\)',
                                stock_text, re.IGNORECASE)

        if not stock_match:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫: –ø—Ä–æ—Å—Ç–æ –∏—â–µ–º –ª—é–±–æ–µ —á–∏—Å–ª–æ
            stock_match = re.search(r'(\d+)', stock_text)

        book_info['stock'] = int(stock_match.group(1)) if stock_match else 0

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–Ω–∏–≥–∞ –≤ –Ω–∞–ª–∏—á–∏–∏
        book_info['in_stock'] = bool(
            stock_elem and
            ('instock' in stock_elem.get('class', []) or
             'available' in stock_text.lower() or
             'in stock' in stock_text.lower())
        )

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É
        link_elem = card.find('h3').find('a')
        href = link_elem.get('href', '') if link_elem else ''

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        if href.startswith('../../../'):
            href = 'catalogue/' + href[9:]
        elif href.startswith('../../'):
            href = 'catalogue/' + href[6:]
        elif not href.startswith('catalogue/'):
            href = 'catalogue/' + href

        book_info['url'] = urljoin(base_url, href)

        return book_info

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–Ω–∏–≥–∏: {e}")
        return None


def parse_book_detail_page(url: str, config: Dict[str, Any],
                           session: requests.Session
                           ) -> Optional[Dict[str, Any]]:
    """
    –ü–∞—Ä—Å–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–¥–µ–ª—å–Ω–æ–π –∫–Ω–∏–≥–∏.

    Args:
        url: URL –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–Ω–∏–≥–∏.
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞.
        session: –°–µ—Å—Å–∏—è requests.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–Ω–∏–≥–µ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
        –í–∫–ª—é—á–∞–µ—Ç –≤—Å–µ –ø–æ–ª—è –∏–∑ parse_book_card() –ø–ª—é—Å:
        {
            'upc': str - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–¥ –ø—Ä–æ–¥—É–∫—Ç–∞,
            'product_type': str - —Ç–∏–ø –ø—Ä–æ–¥—É–∫—Ç–∞,
            'price_excl_tax': float - —Ü–µ–Ω–∞ –±–µ–∑ –Ω–∞–ª–æ–≥–∞,
            'price_incl_tax': float - —Ü–µ–Ω–∞ —Å –Ω–∞–ª–æ–≥–æ–º,
            'tax': float - —Å—É–º–º–∞ –Ω–∞–ª–æ–≥–∞,
            'category': str - –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∫–Ω–∏–≥–∏,
            'image_url': str - URL –æ–±–ª–æ–∂–∫–∏ –∫–Ω–∏–≥–∏
        }
    """

    response = make_request(url, config, session)
    if not response:
        return None

    soup = BeautifulSoup(response.content, 'lxml')

    book_details = {}

    try:
        # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
        title_elem = soup.find('div',
                               class_='product_main').find('h1')
        book_details['title'] = title_elem.text.strip() if title_elem else ''

        # –¶–µ–Ω–∞
        price_elem = soup.find('p', class_='price_color')
        price_text = price_elem.text.strip() if price_elem else ''
        price_match = re.search(r'[\d.]+', price_text)
        book_details['price'] = (float(price_match.group())
                                 if price_match
                                 else 0.0)

        # –ù–∞–ª–∏—á–∏–µ - –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        availability_elem = soup.find('p',
                                      class_='instock availability')

        if not availability_elem:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
            availability_elem = soup.find('p', class_='availability')

        stock_text = (availability_elem.text.strip()
                      if availability_elem
                      else '')

        # –ò—â–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ —Å–∫–æ–±–∫–∞—Ö, –Ω–∞–ø—Ä–∏–º–µ—Ä: "In stock (19 available)"
        stock_match = re.search(r'\((\d+)\s+available\)',
                                stock_text, re.IGNORECASE)

        if not stock_match:
            # –ò—â–µ–º –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ –≤ —Ç–µ–∫—Å—Ç–µ
            stock_match = re.search(r'(\d+)', stock_text)

        book_details['stock'] = (int(stock_match.group(1))
                                 if stock_match
                                 else 0)
        book_details['in_stock'] = bool(
            availability_elem and
            ('instock' in availability_elem.get('class', []) or
             'available' in stock_text.lower() or
             'in stock' in stock_text.lower())
        )

        # –†–µ–π—Ç–∏–Ω–≥
        rating_elem = soup.find('p', class_='star-rating')
        rating_classes = (rating_elem.get('class', [])
                          if rating_elem
                          else [])

        for cls in rating_classes:
            if cls in RATING_MAP:
                book_details['rating'] = RATING_MAP[cls]
                break
        else:
            book_details['rating'] = DEFAULT_RATING

        # UPC –∏ –¥—Ä—É–≥–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
        table = soup.find('table', class_='table table-striped')
        if table:
            for row in table.find_all('tr'):
                header = row.find('th')
                value = row.find('td')
                if header and value:
                    header_text = header.text.strip().lower()
                    value_text = value.text.strip()
                    if 'upc' in header_text:
                        book_details['upc'] = value_text
                    elif 'product type' in header_text:
                        book_details['product_type'] = value_text
                    elif 'price (excl. tax)' in header_text.lower():
                        tax_match = re.search(r'[\d.]+', value_text)
                        book_details['price_excl_tax'] = (
                            float(tax_match.group())
                            if tax_match
                            else 0.0)
                    elif 'price (incl. tax)' in header_text.lower():
                        tax_match = re.search(r'[\d.]+', value_text)
                        book_details['price_incl_tax'] = (
                            float(tax_match.group())
                            if tax_match
                            else 0.0)
                    elif 'tax' in header_text and 'number' not in header_text:
                        tax_match = re.search(r'[\d.]+', value_text)
                        book_details['tax'] = (
                            float(tax_match.group())
                            if tax_match
                            else 0.0)

        # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
        breadcrumb = soup.find('ul', class_='breadcrumb')
        if breadcrumb:
            links = breadcrumb.find_all('a')
            if len(links) >= 3:
                book_details['category'] = links[2].text.strip()

        # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        img_elem = (soup.find('div', class_='item active').find('img')
                    if soup.find('div',
                                 class_='item active')
                    else None)
        if img_elem:
            img_src = img_elem.get('src', '')
            if img_src:
                img_src = img_src.replace('../..', '')
                book_details['image_url'] = urljoin(config['base_url'],
                                                    img_src)

        # URL —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        book_details['url'] = url

        return book_details

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
        return None


def scrape_book_detail(url: str, config: Optional[Dict[str, Any]] = None,
                       session: Optional[requests.Session] = None
                       ) -> Dict[str, Any]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ.

    Args:
        url: URL –∫–Ω–∏–≥–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–º).
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ (—Å–æ–∑–¥–∞–µ—Ç—Å—è –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞).
        session: –°–µ—Å—Å–∏—è requests (—Å–æ–∑–¥–∞–µ—Ç—Å—è –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞).

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–Ω–∏–≥–µ (–ø—É—Å—Ç–æ–π –ø—Ä–∏ –æ—à–∏–±–∫–µ).
    """

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ —Å–µ—Å—Å–∏–∏
    if not config:
        config = get_scraper_config()

    if not session:
        session = requests.Session()
        session.headers.update(config['headers'])

    # –ï—Å–ª–∏ URL –Ω–µ –ø–æ–ª–Ω—ã–π, —Ñ–æ—Ä–º–∏—Ä—É–µ–º –µ–≥–æ
    if not url.startswith('http'):
        if url.startswith('catalogue/'):
            url = f"{config['base_url']}/{url}"
        else:
            url = f"{config['base_url']}/catalogue/{url}"

    logger.debug(f"–ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é parse_book_detail_page, –∫–æ—Ç–æ—Ä—É—é –º—ã —É–∂–µ —Å–æ–∑–¥–∞–ª–∏
    book_details = parse_book_detail_page(url, config, session)

    return book_details or {}


def scrape_page(url: str, config: Dict[str, Any],
                session: requests.Session
                ) -> Tuple[List[Dict[str, Any]], Optional[str]]:
    """
    –ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–∞–ª–æ–≥–∞ –∫–Ω–∏–≥.

    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞.
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞.
        session: –°–µ—Å—Å–∏—è requests.

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (books, next_url):
        - books: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–Ω–∏–≥–∞—Ö –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        - next_url: URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ None –µ—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞

    Workflow:
        1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ make_request()
        2. –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–Ω–∏–≥
        3. –ü–∞—Ä—Å–∏—Ç –∫–∞–∂–¥—É—é –∫–∞—Ä—Ç–æ—á–∫—É —á–µ—Ä–µ–∑ parse_book_card()
        4. –ò—â–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    """

    response = make_request(url, config, session)
    if not response:
        return [], None

    soup = BeautifulSoup(response.content, 'lxml')
    books = []

    # –ò—â–µ–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–Ω–∏–≥
    book_cards = soup.find_all('article', class_='product_pod')

    if not book_cards:
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
        book_cards = soup.select('li.col-xs-6 article')

    # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –∫–∞—Ä—Ç–æ—á–∫—É
    for card in book_cards:
        book_info = parse_book_card(card, config['base_url'])
        if book_info:
            books.append(book_info)

    # –ò—â–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    next_link = soup.find('li', class_='next')
    next_url = None

    if next_link:
        next_a = next_link.find('a')
        if next_a:
            next_href = next_a.get('href', '')
            if next_href:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                if next_href.startswith('catalogue/'):
                    next_url = urljoin(config['base_url'], next_href)
                else:
                    # –ï—Å–ª–∏ —ç—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞ –æ—Ç —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    next_url = urljoin(url, next_href)

    return books, next_url


# ========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –°–û–•–†–ê–ù–ï–ù–ò–Ø –î–ê–ù–ù–´–• ==========

def save_data_locally(books_data: List[Dict[str, Any]],
                      filename: str = LOCAL_CSV_DEFAULT
                      ) -> pd.DataFrame:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π CSV —Ñ–∞–π–ª.

    Args:
        books_data: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–Ω–∏–≥.
        filename: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.

    Returns:
        DataFrame —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–ø—É—Å—Ç–æ–π –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç).
    """
    if not books_data:
        logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return pd.DataFrame()

    df = pd.DataFrame(books_data)
    df.to_csv(filename, index=False, encoding=DEFAULT_ENCODING)

    logger.info(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ª–æ–∫–∞–ª—å–Ω–æ –≤ {filename}")

    return df


def scrape_all_pages(config: Dict[str, Any], max_pages: Optional[int] = None,
                     get_detailed: bool = False
                     ) -> List[Dict[str, Any]]:
    """
    –ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞ –∫–Ω–∏–≥.

    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞.
        max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (None = –≤—Å–µ).
        get_detailed: –ï—Å–ª–∏ True - –ø–∞—Ä—Å–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏.

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤—Å–µ—Ö –∫–Ω–∏–≥–∞—Ö.

    Workflow –ø—Ä–∏ get_detailed=True:
        1. –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ URL –∫–Ω–∏–≥ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        2. –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        3. –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–∞–∑–æ–≤—É—é –∏ –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

    Workflow –ø—Ä–∏ get_detailed=False:
        1. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        2. –°–æ–±–∏—Ä–∞–µ—Ç —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    """

    session = requests.Session()
    session.headers.update(config['headers'])

    books_data = []
    current_url = f"{config['base_url']}/catalogue/page-1.html"
    page_num = 1

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ URL –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    if get_detailed:
        all_books = []
        detail_urls = []

        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–µ–º –≤—Å–µ URL
        while current_url:
            if max_pages and page_num > max_pages:
                break

            books, next_url = scrape_page(current_url, config, session)
            all_books.extend(books)
            detail_urls.extend([book['url'] for book in books])

            logger.info(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {len(books)} –∫–Ω–∏–≥")

            if next_url:
                current_url = next_url
                page_num += 1
                time.sleep(0.1)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            else:
                current_url = None

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        logger.info(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ "
                    f"{len(detail_urls)} –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü...")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        book_dict = {book['url']: book for book in all_books}

        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
        with concurrent.futures.ThreadPoolExecutor(
                max_workers=MAX_WORKERS_VALUE) as executor:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏
            future_to_url = {
                executor.submit(scrape_book_detail,
                                url, config): url
                for url in detail_urls
            }

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    detailed_info = future.result(timeout=10)
                    if detailed_info and url in book_dict:
                        book_dict[url].update(detailed_info)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –¥–ª—è {url}: {e}")

        books_data = list(book_dict.values())
    else:
        # –ë–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –±–µ–∑ –¥–µ—Ç–∞–ª–µ–π
        while current_url:
            if max_pages and page_num > max_pages:
                break

            books, next_url = scrape_page(current_url, config, session)
            books_data.extend(books)

            logger.info(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {len(books)} –∫–Ω–∏–≥")

            if next_url:
                current_url = next_url
                page_num += 1
                time.sleep(0.1)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            else:
                current_url = None

    logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ –∫–Ω–∏–≥: {len(books_data)}")
    return books_data


def full_scrape() -> None:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏ –∞–Ω–∞–ª–∏–∑–æ–º –¥–∞–Ω–Ω—ã—Ö.

    Workflow:
        1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ S3
        2. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–∞–π—Ç–∞
        3. –ü–∞—Ä—Å–∏—Ç –≤—Å–µ –∫–Ω–∏–≥–∏ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω–æ
        5. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ S3
        6. –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        7. –§–∏–ª—å—Ç—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    logger.info("–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ S3...")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º S3
    s3_client, s3_config = initialize_s3()
    scraper_config = get_scraper_config()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å–∞–π—Ç—É
    test_url = f"https://{BASE_URL}/catalogue/page-1.html"
    session = requests.Session()
    session.headers.update(scraper_config['headers'])

    response = make_request(test_url, scraper_config, session)
    if not response:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ —Å–∞–π—Ç—É")
        return

    get_detailed = True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    books_data = scrape_all_pages(scraper_config, get_detailed=get_detailed)

    if books_data:
        logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–æ {len(books_data)} –∫–Ω–∏–≥")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
        filename = LOCAL_CSV_ALL_BOOKS
        if get_detailed:
            filename = LOCAL_CSV_ALL_BOOKS.replace('.csv', '_detailed.csv')

        df = save_data_locally(books_data, filename)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ S3
        if s3_client and s3_config:
            s3_filename = CSV_ALL_BOOKS
            if get_detailed:
                s3_filename = CSV_ALL_BOOKS.replace('.csv', '_detailed.csv')

            s3_key = save_data_to_s3(s3_client,
                                     books_data,
                                     s3_config['bucket_name'],
                                     s3_filename)

            if s3_key:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                analyze_and_save_stats(df, s3_client, s3_config['bucket_name'])

                # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                filter_and_save_books(df,
                                      s3_client=s3_client,
                                      bucket_name=s3_config['bucket_name'])

                logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
                logger.info("–ê–Ω–∞–ª–∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
                logger.info("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ S3!")
                logger.info(f"–°–æ–±—Ä–∞–Ω–æ –∫–Ω–∏–≥: {len(books_data)}")
            else:
                logger.info("–î–∞–Ω–Ω—ã–µ –Ω–µ –±—ã–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ S3, "
                            "–Ω–æ –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ")
    else:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")


def main() -> None:
    """
    –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ. –í—ã–ø–æ–ª–Ω—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞.

    Workflow:
        1. –°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        2. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        3. –í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö
        4. –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ full_scrape()
    """
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(FILES_DIR, exist_ok=True)

    endpoint = os.getenv('SELECTEL_ENDPOINT')
    logger.info("=" * 60)
    logger.info(f"üìö –ü–ê–†–°–ï–† –ö–ù–ò–ì - {BASE_URL}")
    logger.info("üîÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ S3 Selectel")
    logger.info("=" * 60)

    logger.info("\nüìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:")
    logger.info(f"  üìÇ {BASE_DIR}/")
    logger.info(f"    ‚îú‚îÄ‚îÄ üìÅ {os.path.basename(FILES_DIR)}/    "
                f"# CSV —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏")
    logger.info(f"    ‚îú‚îÄ‚îÄ üìÅ {os.path.basename(LOGS_DIR)}/     "
                f"# –õ–æ–≥ —Ñ–∞–π–ª—ã")
    logger.info("    ‚îú‚îÄ‚îÄ üìÅ src/      # –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥")
    logger.info(f"    ‚îî‚îÄ‚îÄ üìÑ {os.path.basename(__file__)}  "
                f"# –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç")

    logger.info("\nüì¶ –¢—Ä–µ–±—É–µ–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
    logger.info("  pip install -r requirements.txt")
    logger.info("\nüîë –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∞–π–ª–∞ .env (–ø—Ä–∏–º–µ—Ä):")
    logger.info("  SELECTEL_ACCESS_KEY=–≤–∞—à_access_key")
    logger.info("  SELECTEL_SECRET_KEY=–≤–∞—à_secret_key")
    logger.info("  SELECTEL_BUCKET=de-books")
    logger.info(f"  SELECTEL_ENDPOINT={endpoint}")
    full_scrape()


if __name__ == "__main__":
    main()
